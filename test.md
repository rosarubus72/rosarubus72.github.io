# ðŸ“ŠðŸš€ 3D-MOOD: Elevating Monocular Object Detection from 2D to 3D ~ï½ž

### ðŸ” Abstract
The abstract section of this research paper addresses the limitations of existing monocular 3D object detection methods, which are primarily designed for closed-set scenarios. These methods struggle when encountering new environments and object categories, a common issue in real-world applications. This paper introduces the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD) to tackle the challenges posed by open-set settings. Key contributions include the development of a 3D bounding box head that lifts 2D detections into 3D space, facilitating joint training for both 2D and 3D tasks. Additionally, the method conditions object queries using geometric priors to enhance generalization across diverse scenes. To further improve performance, the authors design a canonical image space for more efficient cross-dataset training. The evaluation of 3D-MOOD demonstrates its effectiveness on various datasets, showcasing significant advancements in handling open-set monocular 3D object detection.

![Open-set Monocular 3D Object Detection. Unlike previous methods focusing on achieving good results i](assets/section_0_3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection-picture-1.jpg)
> Open-set Monocular 3D Object Detection. Unlike previous methods focusing on achieving good results in the closedset setting, we aim to resolve the open-set monocular 3D object detection problem. This challenge requires the model to classify arbitrary objects while precisely localizing them in unseen scenes.

### ðŸš€ Motivation
The primary motivation for this research is to address the limitations of existing monocular 3D object detection (3DOD) methods, which are predominantly designed for closed-set scenarios. These methods excel in specific domains but struggle to generalize to novel scenes and object categories. Specifically, they are constrained by the closed-set classification paradigm, which assumes all possible object categories are known during both training and testing phases. This assumption significantly hinders their applicability in real-world scenarios where novel objects and scenes frequently emerge.

To enhance the robustness and versatility of 3DOD systems, it is imperative to develop an open-set 3DOD framework. Such a system would be capable of detecting both known and novel objects and scenes, thereby expanding its practical utility and generalizability. This research aims to bridge the gap between current closed-set 3DOD methods and the demands of real-world applications, particularly in autonomous driving and indoor robotic navigation, where the ability to handle diverse and unknown environments is critical. By addressing these limitations, we can advance 3DOD technology to meet the challenges of dynamic and unpredictable real-world scenarios.

![3D-MOOD. We propose an end-to-end 3D monocular open-set object detector that takes a monocular image](assets/section_1_3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection-picture-2.jpg)
> 3D-MOOD. We propose an end-to-end 3D monocular open-set object detector that takes a monocular image and the language prompts of the interested objects as input and classifies and localizes the 3D objects in the scenes. Our design will transform the input image and camera intrinsics into the proposed canonical image space and achieve the open-set ability for diverse scenes.

### ðŸ’¡ Innovation
The innovation section of this research paper highlights the development of 3D-MOOD, an end-to-end 3D monocular open-set object detection method. This approach introduces several novel components that significantly enhance its performance compared to existing methods. Key contributions include:

- **Canonical Image Space**: 3D-MOOD transforms the input image and camera intrinsics into a canonical space, which facilitates robust performance across various scenes and novel object classes. This transformation allows the model to generalize better to unseen object categories, addressing the limitations of traditional closed-set designs.

- **Geometry-Aware 3D Query Generation**: The method employs a geometry-aware 3D query generation mechanism that leverages the spatial relationships between objects in the scene. This mechanism ensures that the model can accurately detect and localize objects even in complex environments, improving overall detection accuracy.

- **Auxiliary Metric Depth Estimation**: An additional auxiliary metric depth estimation head is incorporated, which is conditioned on camera information. This component enhances the modelâ€™s ability to handle varying depths and perspectives, further boosting its generalizability across different datasets.

These innovations collectively enable 3D-MOOD to outperform existing methods on both closed-set and open-set benchmarks, setting new state-of-the-art performance on the Omni3D and open-set benchmarks. The end-to-end trainable architecture of 3D-MOOD also simplifies the integration process and facilitates more efficient training and deployment.

![Open-set Results . We propose the first 3D monocular open-set object detection benchmark with Argove](assets/section_2_3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection-table-1.jpg)
> Open-set Results . We propose the first 3D monocular open-set object detection benchmark with Argoverse 2 [54] (Outdoor) and ScanNet [8] (Indoor). Each dataset contains seen (base) and unseen (novel) categories in the unseen scenes. Besides Cube R-CNN [4] full model, we evaluate Cube R-CNN (In/Out) as each domain expert variant, which is only trained and tested on Omni3D indoor/outdoor datasets. It is worth noting that OVM3D-Det's depth estimation model [38] is trained on AV2 and ScanNet. We further evaluate the generalization of seen classes and the ability to detect novel classes through ODS (B) and ODS (N) . 3D-MOOD establishes the SOTA performance on this new challenging open-set benchmark.

### ðŸ› ï¸ Methodology
The methodology section of this research paper introduces the development of 3D-MOOD, an end-to-end framework for monocular open-set 3D object detection. The approach begins with the utilization of G-DINO, an open-set 2D object detector, which predicts 2D bounding boxes from input images and language prompts. These 2D bounding boxes are subsequently transformed into 3D space, generating 3D oriented bounding boxes in the camera coordinate frame.

Key components of the 3D-MOOD architecture include:
- **3D Bounding Box Head**: This component employs a Feature Pyramid Network (FPN) to extract depth features, which are conditioned on camera embeddings to achieve robust monocular depth estimation. This ensures the modelâ€™s ability to generalize across various datasets.
- **Canonical Image Space**: This module facilitates the alignment of 3D bounding boxes with the canonical view, enhancing the accuracy of the detection process.
- **Geometry-Aware 3D Query Generation Module**: This module generates queries that are sensitive to geometric properties, thereby improving the modelâ€™s performance in both closed-set and open-set scenarios.

Additionally, the method introduces a metric monocular auxiliary depth head to enhance the understanding of the global scene, providing supplementary depth information that complements the main depth estimation process. This multi-faceted approach ensures that 3D-MOOD can effectively handle the complexities of open-set scenarios, where the model must detect objects it has not seen during training.

![Ablations of 3D-MOOD . CI denotes canonical image space, Depth denotes auxiliary depth estimation he](assets/section_3_3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection-table-4.jpg)
> Ablations of 3D-MOOD . CI denotes canonical image space, Depth denotes auxiliary depth estimation head, and GA stands for geometry-aware 3D query generation. We report the IoU-based AP for the Omni3D test split and our ODS for the AV2 and ScanNet validation split. AP omni 3D is the average scores over Omni3D 6 datasets while ODS open is the average for open-set datasets. The results show that our proposed component help for both closed-set and open-set settings.

### ðŸ“Š Experiments
The experiments section evaluated the performance of 3D-MOOD across various scenarios, including open-set, cross-domain, and closed-set settings. Key findings demonstrated that 3D-MOOD outperformed existing methods in terms of open-set performance, achieving state-of-the-art results on challenging datasets. Specifically, in the cross-domain scenario, 3D-MOOD showed consistent improvement across different datasets, with an average precision (AP) of 25.6, 15.9, and 14.5 for Hypersim, SUN RGB-D, and ARKitScenes, respectively, surpassing baselines such as Cube R-CNN and Uni-MODE. In the closed-set setting, 3D-MOOD also performed well, achieving superior results on the Omni3D benchmark, with an AP of 28.4 and 30 when using Swin-T and Swin-B models, respectively.

These results highlight the effectiveness of 3D-MOODâ€™s geometry-aware 3D query generation and its ability to resolve ambiguities in image and depth spaces, thereby enhancing its generalization capabilities to unseen scenes. The experiments further confirmed that 3D-MOOD can effectively handle open-set scenarios by leveraging its unique framework, which integrates geometric reasoning into the detection process. This approach not only improves the robustness of the model but also enhances its ability to detect novel objects without prior training data. The consistent improvements across different datasets underscore the versatility and reliability of 3D-MOOD in diverse real-world applications.

![Results on Omni3D. We compare 3D-MOOD with other closed-set detectors on Omni3D test set. AP omni 3D](assets/section_4_3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection-table-3.jpg)
> Results on Omni3D. We compare 3D-MOOD with other closed-set detectors on Omni3D test set. AP omni 3D â†‘ is the average scores over Omni3D 6 datasets. All methods are trained with Omni3D train and val splits and '-' represents the results not reported in previous literature [4, 23]. 3D-MOOD achieves SOTA performance on the closed-set setting with the open-set ability.

